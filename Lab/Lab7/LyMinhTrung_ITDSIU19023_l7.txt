import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial.distance import cdist
np.random.seed(11) 

means = [[2, 2], [8, 3], [3, 6]]
cov = [[1, 0], [0, 1]]
N = 100000
X0 = np.random.multivariate_normal(means[0], cov, N)
X1 = np.random.multivariate_normal(means[1], cov, N)
X2 = np.random.multivariate_normal(means[2], cov, N)
X = np.concatenate((X0, X1, X2), axis = 0)
K = 3
original_label = np.asarray([0]*N + [1]*N + [2]*N).T


def kmeans_display(X, label):
    K = np.amax(label) + 1
    X0 = X[label == 0, :]
    X1 = X[label == 1, :]
    X2 = X[label == 2, :]
    
    plt.plot(X0[:, 0], X0[:, 1], 'b^', markersize = 4, alpha = .8)
    plt.plot(X1[:, 0], X1[:, 1], 'go', markersize = 4, alpha = .8)
    plt.plot(X2[:, 0], X2[:, 1], 'rs', markersize = 4, alpha = .8)

    plt.axis('equal')
    plt.plot()
    plt.show()
    
kmeans_display(X, original_label)

def kmeans_display(X, label):
 K = np.amax(label) + 1
 X0 = X[label == 0, :]
 X1 = X[label == 1, :]
 X2 = X[label == 2, :]
 plt.plot(X0[:, 0], X0[:, 1], 'b^', markersize = 4, alpha = .8)
 plt.plot(X1[:, 0], X1[:, 1], 'go', markersize = 4, alpha = .8)
 plt.plot(X2[:, 0], X2[:, 1], 'rs', markersize = 4, alpha = .8)
 plt.axis('equal')
 plt.plot()
 plt.show()
kmeans_display(X, original_label)

def kmeans_init_centers(X, k):
    # randomly pick k rows of X as initial centers
    return X[np.random.choice(X.shape[0], k, replace=False)]

def kmeans_assign_labels(X, centers):
    # calculate pairwise distances btw data and centers
    D = cdist(X, centers)
    # return index of the closest center
    return np.argmin(D, axis = 1)

def kmeans_update_centers(X, labels, K):
    centers = np.zeros((K, X.shape[1]))
    for k in range(K):
        # collect all points assigned to the k-th cluster 
        Xk = X[labels == k, :]
        # take average
        centers[k,:] = np.mean(Xk, axis = 0)
    return centers

def has_converged(centers, new_centers):
    # return True if two sets of centers are the same
    return (set([tuple(a) for a in centers]) == 
        set([tuple(a) for a in new_centers]))


def kmeans(X, K):
    centers = [kmeans_init_centers(X, K)]
    labels = []
    it = 0 
    while True:
        labels.append(kmeans_assign_labels(X, centers[-1]))
        new_centers = kmeans_update_centers(X, labels[-1], K)
        if has_converged(centers[-1], new_centers):
            break
        centers.append(new_centers)
        it += 1 
        kmeans_display(X, labels[-1])
    return (centers, labels, it)


(centers, labels, it) = kmeans(X, K)
print('Centers found by our algorithm:')
print(centers[-1])

kmeans_display(X, labels[-1])


 The system will range the set with diffenrent number iteration in each case. 
normally in 5-6 iterations
      Example: 
[[1.99608062 1.98718382]
 [7.99912704 2.99288802]
 [3.00445129 6.01711559]] 

change: 
[[2.06150003 2.00629087]
[7.98752557 3.11437196]
[3.05051973 6.10009562]] 
 then: 
[[2.05599928 1.99324946]
[8.02459351 3.08640712]
[3.08242542 6.0746446611]]   




From this result, we see that the K-means clustering algorithm works quite successfully, 
the centers found are quite close to the original expectation. Points belonging 
to the same cluster are mostly assigned to the same cluster

Next, we create the data by taking the points according to the expected normal 
distribution at the points with the coordinates (2, 2), (8, 3) and (3, 6), 
the same covariance matrix and is the unit matrix. Each cluster has 500 points. 
(Note that each data point is a row of the data matrix.

Contrary to supervised learning where we have the ground truth to evaluate the model’s performance, clustering analysis doesn’t have a solid evaluation metric that we can use to evaluate the outcome of different clustering algorithms. Moreover, since kmeans requires k as an input and doesn’t learn it from data, there is no right answer in terms of the number of clusters that we should have in any problem. Sometimes domain knowledge and intuition may help but usually that is not the case. In the cluster-predict methodology, we can evaluate how well the models are performing based on different K clusters since clusters are used in the downstream modeling.
In this post we’ll cover two metrics that may give us some intuition about k:
Elbow method
Silhouette analysis